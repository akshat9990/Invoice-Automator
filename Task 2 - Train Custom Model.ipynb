{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the custom Named Entity Recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SfaMMD1-Rzcs",
    "outputId": "74cde306-8261-41b8-de48-eed573337e8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "euP7-1Qd4VLx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Iyrzu8sNAS99"
   },
   "outputs": [],
   "source": [
    "text = os.listdir(\"/content/drive/MyDrive/text\")  \n",
    "entities = os.listdir(\"/content/drive/MyDrive/entities\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YSzT8vZOTY5U",
    "outputId": "cf2ded29-c877-449e-cd23-1d3e7759e643"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "716\n",
      "716\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(len(entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstructing strings for training the model\n",
    "- The reciept text is extracted and reconstructed into strings from `Task 1 - Scanned Reciept Localization` training data\n",
    "- The true values for entities in the exracted text in taken from `Task 2 - Scanned Reciept OCR` training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gMkCyay4AS6t"
   },
   "outputs": [],
   "source": [
    "set_text  = set(text)\n",
    "set_ent = set(entities)\n",
    "\n",
    "training_set = list(set_text.intersection(set_ent)) #to avoid mismatch between text and entity dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fUmsVAVq5lCD",
    "outputId": "b21bdd28-1a89-402e-9fbf-96fe699a85d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(716, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(columns=[\"filename\", \"text\"])\n",
    "\n",
    "data[\"filename\"] = training_set\n",
    "\n",
    "data_text = []\n",
    "for file in data[\"filename\"]:\n",
    "    rec_text = []\n",
    "    pattern = r\"\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,(.+)\"\n",
    "    with open(f\"/content/drive/MyDrive/text/{file}\") as f:\n",
    "        f.seek(0)\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            rec_text += re.findall(pattern, line)\n",
    "    data_text.append(\" \".join([x.strip() for x in rec_text]))\n",
    "data[\"text\"] = data_text\n",
    "\n",
    "ent_list = []\n",
    "for file in data[\"filename\"]:\n",
    "    with open(f\"/content/drive/MyDrive/Entities/{file}\") as f:\n",
    "        entity_dict = json.load(f)\n",
    "        ent_list.append(entity_dict)\n",
    "data[\"entity_dictionary\"] = ent_list\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "id": "h32Jn2iE5k_a",
    "outputId": "dc92e60a-4991-4235-a3a5-1355022b6137"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>entity_dictionary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X51007339157(1).txt</td>\n",
       "      <td>SANYU STATIONERY SHOP NO. 31G&amp;33G, JALAN SETIA...</td>\n",
       "      <td>{'company': 'SANYU STATIONERY SHOP', 'date': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X51005763940(1).txt</td>\n",
       "      <td>HARVEY NORMAN HARVEY NORMAN M'SIA PARADIGM MAL...</td>\n",
       "      <td>{'company': 'ELITETRAX MARKETING SDN BHD', 'da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X51005757199.txt</td>\n",
       "      <td>POPULAR BOOK CO. (M) SDN BHD (CO. NO. 113825-W...</td>\n",
       "      <td>{'company': 'POPULAR BOOK CO. (M) SDN BHD', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X51008142063.txt</td>\n",
       "      <td>KEDAI PAPAN YEW CHUAN (0005583085-K) LOT 276 J...</td>\n",
       "      <td>{'company': 'KEDAI PAPAN YEW CHUAN', 'date': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X51005447861.txt</td>\n",
       "      <td>POPULAR BOOK CO. (M) SDN BHD (CO. NO. 113825-W...</td>\n",
       "      <td>{'company': 'POPULAR BOOK CO. (M) SDN BHD', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>X51005568827.txt</td>\n",
       "      <td>BANH MI CAFE DIMILIKI: BANH MI CAFE SDN BHD 11...</td>\n",
       "      <td>{'company': 'BANH MI CAFE SDN BHD', 'date': '2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>X51005677331.txt</td>\n",
       "      <td>SYARIKAT PERNIAGAAN GIN KEE (81109-A) NO 290, ...</td>\n",
       "      <td>{'company': 'SYARIKAT PERNIAGAAN GIN KEE', 'da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>X51005442338.txt</td>\n",
       "      <td>PASAR MINI JIN SENG 379,JALAN PERMAS SATU, BAN...</td>\n",
       "      <td>{'company': 'PASAR MINI JIN SENG', 'date': '18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>X51006414483.txt</td>\n",
       "      <td>UNIHAKKA INTERNATIONAL SDN BHD 10 APR 2018 18:...</td>\n",
       "      <td>{'company': 'UNIHAKKA INTERNATIONAL SDN BHD', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>X51006619854.txt</td>\n",
       "      <td>99 SPEED MART S/B (519537-X) LOT P.T. 33198, B...</td>\n",
       "      <td>{'company': '99 SPEED MART S/B', 'date': '04-0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>716 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                filename  ...                                  entity_dictionary\n",
       "0    X51007339157(1).txt  ...  {'company': 'SANYU STATIONERY SHOP', 'date': '...\n",
       "1    X51005763940(1).txt  ...  {'company': 'ELITETRAX MARKETING SDN BHD', 'da...\n",
       "2       X51005757199.txt  ...  {'company': 'POPULAR BOOK CO. (M) SDN BHD', 'd...\n",
       "3       X51008142063.txt  ...  {'company': 'KEDAI PAPAN YEW CHUAN', 'date': '...\n",
       "4       X51005447861.txt  ...  {'company': 'POPULAR BOOK CO. (M) SDN BHD', 'd...\n",
       "..                   ...  ...                                                ...\n",
       "711     X51005568827.txt  ...  {'company': 'BANH MI CAFE SDN BHD', 'date': '2...\n",
       "712     X51005677331.txt  ...  {'company': 'SYARIKAT PERNIAGAAN GIN KEE', 'da...\n",
       "713     X51005442338.txt  ...  {'company': 'PASAR MINI JIN SENG', 'date': '18...\n",
       "714     X51006414483.txt  ...  {'company': 'UNIHAKKA INTERNATIONAL SDN BHD', ...\n",
       "715     X51006619854.txt  ...  {'company': '99 SPEED MART S/B', 'date': '04-0...\n",
       "\n",
       "[716 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the data to be trained for our spaCy model\n",
    "\n",
    "For training a spaCy model for customized named entity recognition, the data must be transformed to the following format<br>\n",
    "`[(input text, entites:[(start_index, end_index, entity_name), (start_index, end_index, entity_name), ...]), ....]` \n",
    "<br>\n",
    "To correcty match the entity values given in the entity dictionary to the phrases in the corresponding text file, I have used:\n",
    "- Phrase Matcher\n",
    "- Regular Expressions\n",
    "Regular expressions are used to identify and match the phrases/tokens that were not matched by the Phrase Matcher to ensure the training data set to be as effective as possible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Iq1cxRhG5k8q"
   },
   "outputs": [],
   "source": [
    "training_data = []\n",
    "id_ent = []\n",
    "\n",
    "nlp_match = spacy.load('en_core_web_sm')\n",
    "matcher = PhraseMatcher(nlp_match.vocab)\n",
    "for index, row in data.iterrows():\n",
    "    ent_dic = row[\"entity_dictionary\"]\n",
    "    ent = []\n",
    "    phrases = list(ent_dic.values())\n",
    "    patterns = [nlp_match.make_doc(phrase) for phrase in phrases]\n",
    "    matcher.add(\"EntityList\", None, *patterns)\n",
    "\n",
    "    doc = nlp_match(row[\"text\"])\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        try:\n",
    "            span = doc[start:end]\n",
    "            if start>0:\n",
    "                sb = doc[0:start]\n",
    "                start_index=len(sb.text)+1\n",
    "            else:\n",
    "                start_index=0\n",
    "            end_index= start_index+len(span.text)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for key, value in ent_dic.items():\n",
    "            if value==span.text:\n",
    "                ent_tup=(start_index, end_index, key)\n",
    "                ent.append(ent_tup)\n",
    "    ent_set = {\"company\", \"date\", \"total\", \"address\"}\n",
    "    detected_entities = set([key for start, end, key in ent])\n",
    "    missed_entities = list(ent_set - detected_entities)\n",
    "    if \"total\" in missed_entities:\n",
    "        value = ent_dic[\"total\"]\n",
    "        if len(value)>0:\n",
    "            catch_total = re.search(value, str(row[\"text\"]).replace(\",\", \"\"))\n",
    "            ent_tup = (catch_total.span()[0], catch_total.span()[1], \"total\")\n",
    "            ent.append(ent_tup)\n",
    "    if \"date\" in missed_entities:\n",
    "        value = ent_dic[\"date\"]\n",
    "        if len(value)>0:\n",
    "            catch_date = re.search(value, str(row[\"text\"]))\n",
    "            if catch_date == None:\n",
    "                catch_date = re.search(r\"\\d\\d[-/]*\\d\\d[-/]*\\d\\d\", str(row[\"text\"]))\n",
    "            try:\n",
    "                ent_tup = (catch_total.span()[0], catch_total.span()[1], \"date\")\n",
    "                ent.append(ent_tup)\n",
    "            except:\n",
    "                pass\n",
    "    if \"company\" in missed_entities:\n",
    "        value = ent_dic[\"company\"]\n",
    "        catch_company = re.search(value, str(row[\"text\"]))\n",
    "        if catch_company!=None:\n",
    "            ent_tup = (catch_company.span()[0], catch_company.span()[1], \"company\")\n",
    "            ent.append(ent_tup)\n",
    "        else:\n",
    "            catch_company = re.search(value, str(row[\"text\"]).replace(\".\", \"\"))\n",
    "            if catch_company!=None:\n",
    "                ent_tup = (catch_company.span()[0], catch_company.span()[1], \"company\")\n",
    "                ent.append(ent_tup)\n",
    "    if \"address\" in missed_entities:\n",
    "        try:\n",
    "            value = ent_dic[\"address\"]\n",
    "            catch_address = re.search(value, str(row[\"text\"]))\n",
    "            if catch_address!=None:\n",
    "                ent_tup = (catch_address.span()[0], catch_address.span()[1], \"address\")\n",
    "                ent.append(ent_tup)\n",
    "        except:\n",
    "            pass\n",
    "    id_ent.append(len(ent))\n",
    "    entity_dictionary = {\"entities\": ent}\n",
    "    train_tup = (row[\"text\"], entity_dictionary)\n",
    "    training_data.append(train_tup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oqg02Tu963rO",
    "outputId": "0a64b6b6-476f-4435-89e6-253a3df7eb06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "716"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the custom NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class NER:\n",
    "    def __init__(\n",
    "        self,\n",
    "        iter=100,\n",
    "        dropout=0.5,\n",
    "        min_batchsize=4.0,\n",
    "        max_batchsize=32.0,\n",
    "        compounding_coef=0.01,\n",
    "        train_text_path=None,\n",
    "        train_entities_path=None,\n",
    "        test_text_path=None,\n",
    "    ):\n",
    "        self.iter = iter\n",
    "        self.dropout = dropout\n",
    "        self.min_batchsize = min_batchsize\n",
    "        self.max_batchsize = max_batchsize\n",
    "        self.compounding_coef = compounding_coef\n",
    "        self.train_text_path = train_text_path\n",
    "        self.train_entities_path = train_entities_path\n",
    "        self.test_text_path = test_text_path\n",
    "\n",
    "    def get_data(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Get data from the text files and transform into a pandas dataframe \n",
    "        parameters: None\n",
    "        returns: dataframe\n",
    "        \"\"\"\n",
    "        set_text = set(os.listdir(self.train_text_path))\n",
    "        set_ent = set(os.listdir(self.train_entities_path))\n",
    "        training_set = list(set_text.intersection(set_ent))\n",
    "\n",
    "        self.data = pd.DataFrame(columns=[\"filename\", \"text\"])\n",
    "        self.data[\"filename\"] = training_set\n",
    "        data_text = []\n",
    "        for file in self.data[\"filename\"]:\n",
    "            rec_text = []\n",
    "            pattern = r\"\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,(.+)\"\n",
    "            with open(os.path.join(self.train_text_path, file)) as f:\n",
    "                f.seek(0)\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    rec_text += re.findall(pattern, line)\n",
    "            data_text.append(\" \".join([x.strip() for x in rec_text]))\n",
    "        self.data[\"text\"] = data_text\n",
    "        ent_list = []\n",
    "        for file in self.data[\"filename\"]:\n",
    "            with open(f\"{self.train_entities_path}/{file}\") as f:\n",
    "                entity_dict = json.load(f)\n",
    "                ent_list.append(entity_dict)\n",
    "        self.data[\"entity_dictionary\"] = ent_list\n",
    "\n",
    "        return self.data\n",
    "\n",
    "    def transform_data(self, data, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Transform pandas dataframe to the spaCy compliant training data format \n",
    "        parameters: DataFrame\n",
    "        returns: List of text and entity tuples\n",
    "        \"\"\"\n",
    "        training_data = []\n",
    "        id_ent = []\n",
    "        nlp_match = spacy.load(\"en_core_web_sm\")\n",
    "        matcher = PhraseMatcher(nlp_match.vocab)\n",
    "        for index, row in self.data.iterrows():\n",
    "            ent_dic = row[\"entity_dictionary\"]\n",
    "            ent = []\n",
    "            phrases = list(ent_dic.values())\n",
    "            patterns = [nlp_match.make_doc(phrase) for phrase in phrases]\n",
    "            matcher.add(\"EntityList\", None, *patterns)\n",
    "\n",
    "            doc = nlp_match(row[\"text\"])\n",
    "            matches = matcher(doc)\n",
    "            for match_id, start, end in matches:\n",
    "                try:\n",
    "                    span = doc[start:end]\n",
    "                    if start > 0:\n",
    "                        sb = doc[0:start]\n",
    "                        start_index = len(sb.text) + 1\n",
    "                    else:\n",
    "                        start_index = 0\n",
    "                    end_index = start_index + len(span.text)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                for key, value in ent_dic.items():\n",
    "                    if value == span.text:\n",
    "                        ent_tup = (start_index, end_index, key)\n",
    "                        ent.append(ent_tup)\n",
    "            ent_set = {\"company\", \"date\", \"total\", \"address\"}\n",
    "            detected_entities = set([key for start, end, key in ent])\n",
    "            missed_entities = list(ent_set - detected_entities)\n",
    "            if \"total\" in missed_entities:\n",
    "                value = ent_dic[\"total\"]\n",
    "                if len(value) > 0:\n",
    "                    catch_total = re.search(value, str(row[\"text\"]).replace(\",\", \"\"))\n",
    "                    ent_tup = (catch_total.span()[0], catch_total.span()[1], \"total\")\n",
    "                    ent.append(ent_tup)\n",
    "            if \"date\" in missed_entities:\n",
    "                value = ent_dic[\"date\"]\n",
    "                if len(value) > 0:\n",
    "                    catch_date = re.search(value, str(row[\"text\"]))\n",
    "                    if catch_date == None:\n",
    "                        catch_date = re.search(\n",
    "                            r\"\\d\\d[-/]*\\d\\d[-/]*\\d\\d\", str(row[\"text\"])\n",
    "                        )\n",
    "                    try:\n",
    "                        ent_tup = (catch_total.span()[0], catch_total.span()[1], \"date\")\n",
    "                        ent.append(ent_tup)\n",
    "                    except:\n",
    "                        pass\n",
    "            if \"company\" in missed_entities:\n",
    "                value = ent_dic[\"company\"]\n",
    "                catch_company = re.search(value, str(row[\"text\"]))\n",
    "                if catch_company != None:\n",
    "                    ent_tup = (\n",
    "                        catch_company.span()[0],\n",
    "                        catch_company.span()[1],\n",
    "                        \"company\",\n",
    "                    )\n",
    "                    ent.append(ent_tup)\n",
    "                else:\n",
    "                    catch_company = re.search(value, str(row[\"text\"]).replace(\".\", \"\"))\n",
    "                    if catch_company != None:\n",
    "                        ent_tup = (\n",
    "                            catch_company.span()[0],\n",
    "                            catch_company.span()[1],\n",
    "                            \"company\",\n",
    "                        )\n",
    "                        ent.append(ent_tup)\n",
    "            if \"address\" in missed_entities:\n",
    "                try:\n",
    "                    value = ent_dic[\"address\"]\n",
    "                    catch_address = re.search(value, str(row[\"text\"]))\n",
    "                    if catch_address != None:\n",
    "                        ent_tup = (\n",
    "                            catch_address.span()[0],\n",
    "                            catch_address.span()[1],\n",
    "                            \"address\",\n",
    "                        )\n",
    "                        ent.append(ent_tup)\n",
    "                except:\n",
    "                    pass\n",
    "            id_ent.append(len(ent))\n",
    "            entity_dictionary = {\"entities\": ent}\n",
    "            train_tup = (row[\"text\"], entity_dictionary)\n",
    "            training_data.append(train_tup)\n",
    "\n",
    "        return training_data\n",
    "\n",
    "    def fit(self, train_text_path, train_entities_path, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Fit a blank English language model from spaCy and save the model in the current directory \n",
    "        parameters: None\n",
    "        returns: None\n",
    "        \"\"\"\n",
    "        self.train_text_path = train_text_path\n",
    "        self.train_entities_path = train_entities_path\n",
    "        data = self.get_data(self)\n",
    "        training_data = self.transform_data(self, data)\n",
    "        TRAIN_DATA = training_data\n",
    "        output_dir = os.path.join(os.getcwd(), \"model\")\n",
    "\n",
    "        nlp = spacy.blank(\"en\")\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "        for _, annotations in TRAIN_DATA:\n",
    "            for ent in annotations.get(\"entities\"):\n",
    "                ner.add_label(ent[2])\n",
    "\n",
    "        dropout = self.dropout\n",
    "        min_batchsize = self.min_batchsize\n",
    "        max_batchsize = self.max_batchsize\n",
    "        compounding_coef = self.compounding_coef\n",
    "        n_iter = self.iter\n",
    "\n",
    "        nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            batches = minibatch(\n",
    "                TRAIN_DATA,\n",
    "                size=compounding(min_batchsize, max_batchsize, compounding_coef),\n",
    "            )\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                try:\n",
    "                    nlp.update(\n",
    "                        texts, annotations, drop=dropout, losses=losses,\n",
    "                    )\n",
    "                except:\n",
    "                    pass\n",
    "            print(f\"{itn} Losses\", losses)\n",
    "\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "\n",
    "    def get_test_data(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Fetch the data from the test directory and transform it into a pandas DataFrame\n",
    "        returns: DataFrame \n",
    "        \"\"\"\n",
    "        test_text_files = os.listdir(self.test_text_path)\n",
    "        test_data = pd.DataFrame(columns=[\"filename\", \"text\"])\n",
    "        test_data[\"filename\"] = test_text_files\n",
    "        data_text = []\n",
    "        for file in test_data[\"filename\"]:\n",
    "            rec_text = []\n",
    "            pattern = r\"\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,\\d+,(.+)\"\n",
    "            try:\n",
    "                with open(os.path.join(self.test_text_path, file)) as f:\n",
    "                    lines = f.readlines()\n",
    "                    for line in lines:\n",
    "                        rec_text += re.findall(pattern, line)\n",
    "            except:\n",
    "                pass\n",
    "            data_text.append(\" \".join([x.strip() for x in rec_text]))\n",
    "        test_data[\"text\"] = data_text\n",
    "        return test_data\n",
    "\n",
    "    def predict(self, test_text_path, reciept_text=\"\", is_dir=True, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Identify entities in a new text string\n",
    "        \"\"\"\n",
    "        cwd = os.getcwd()\n",
    "        nlp = spacy.load(os.path.join(cwd, \"model\"))\n",
    "        train = os.listdir(self.train_entities_path)\n",
    "        ent_list = []\n",
    "        for file in train:\n",
    "            with open(os.path.join(self.train_entities_path, file)) as f:\n",
    "                entity_dict = json.load(f)\n",
    "                ent_list.append(entity_dict)\n",
    "        memory_dictionary = {\"company\": [], \"address\": [], \"date\": [], \"total\": []}\n",
    "        for dictionary in ent_list:\n",
    "            for key, value in dictionary.items():\n",
    "                memory_dictionary[key].append(value)\n",
    "\n",
    "        if is_dir:\n",
    "            self.test_text_path = test_text_path\n",
    "            test_data = self.get_test_data(self)\n",
    "            for index, row in test_data.iterrows():\n",
    "                op_dict = {\"company\": \"\", \"date\": \"\", \"address\": \"\", \"total\": \"\"}\n",
    "                doc = nlp(row[\"text\"])\n",
    "                for ent in doc.ents:\n",
    "                    op_dict[ent.label_] = ent.text\n",
    "\n",
    "                for tag, tag_memory in memory_dictionary.items():\n",
    "                    for tag_value in tag_memory:\n",
    "                        if tag == \"total\":\n",
    "                            pass\n",
    "                        elif (re.search(tag_value, row[\"text\"]) != None) and (\n",
    "                            op_dict[tag] == \"\"\n",
    "                        ):\n",
    "                            op_dict[tag] = tag_value\n",
    "\n",
    "                print(\"Entities: \", op_dict)\n",
    "                op_dir = os.path.join(cwd, \"output\")\n",
    "                if not os.path.isdir(op_dir):\n",
    "                    os.mkdir(path=op_dir)\n",
    "                json_object = json.dumps(op_dict, indent=4)\n",
    "                with open(os.path.join(op_dir, f\"{row['filename']}\"), \"w\") as op:\n",
    "                    op.write(json_object)\n",
    "        else:\n",
    "            doc = nlp(reciept_text)\n",
    "            op_dict = {\"company\": \"\", \"date\": \"\", \"address\": \"\", \"total\": \"\"}\n",
    "            for ent in doc.ents:\n",
    "                op_dict[ent.label_] = ent.text\n",
    "            for tag, tag_memory in memory_dictionary.items():\n",
    "                for tag_value in tag_memory:\n",
    "                    if tag == \"total\":\n",
    "                        pass\n",
    "                    elif (re.search(tag_value, row[\"text\"]) != None) and (\n",
    "                        op_dict[tag] == \"\"\n",
    "                    ):\n",
    "                        op_dict[tag] = tag_value\n",
    "            print(\"Entities: \", op_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparmeter Values:\n",
    "- **Number of Iterations**: 80\n",
    "- **Dropout**: 0.6\n",
    "- **Minimum Batch Size**: 4\n",
    "- **Maximum Batch Size**: 32\n",
    "- **Compounding factor**: 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YVTRlu0DASxu"
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA = training_data\n",
    "output_dir=\"/content/drive/MyDrive/Invoice_Automator/model\"\n",
    "n_iter = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tl35KCaxDIni",
    "outputId": "7c8306fd-ddd7-4d33-9f8d-d83338826781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Losses {'ner': 12974.743664056032}\n",
      "1 Losses {'ner': 17605.0854382627}\n",
      "2 Losses {'ner': 22397.8668254816}\n",
      "3 Losses {'ner': 22230.640010044022}\n",
      "4 Losses {'ner': 23507.19024447282}\n",
      "5 Losses {'ner': 23773.06150963734}\n",
      "6 Losses {'ner': 22811.54948925385}\n",
      "7 Losses {'ner': 19172.24888464052}\n",
      "8 Losses {'ner': 17384.020578325573}\n",
      "9 Losses {'ner': 18561.810039345055}\n",
      "10 Losses {'ner': 17603.17206580461}\n",
      "11 Losses {'ner': 17206.517447793653}\n",
      "12 Losses {'ner': 14790.336147851283}\n",
      "13 Losses {'ner': 16355.159918238129}\n",
      "14 Losses {'ner': 13728.183245581147}\n",
      "15 Losses {'ner': 13496.02545978631}\n",
      "16 Losses {'ner': 13030.180207959613}\n",
      "17 Losses {'ner': 11069.342757673901}\n",
      "18 Losses {'ner': 9289.0420544195}\n",
      "19 Losses {'ner': 8272.538787234234}\n",
      "20 Losses {'ner': 8160.740559127125}\n",
      "21 Losses {'ner': 8738.867668556524}\n",
      "22 Losses {'ner': 5865.906072477139}\n",
      "23 Losses {'ner': 5362.072545458739}\n",
      "24 Losses {'ner': 5091.726134207575}\n",
      "25 Losses {'ner': 4252.640293221}\n",
      "26 Losses {'ner': 4769.905745454959}\n",
      "27 Losses {'ner': 3983.477331944797}\n",
      "28 Losses {'ner': 4075.0341932910296}\n",
      "29 Losses {'ner': 4035.938071442165}\n",
      "30 Losses {'ner': 4218.489610879166}\n",
      "31 Losses {'ner': 3294.162067412498}\n",
      "32 Losses {'ner': 3160.5214823200054}\n",
      "33 Losses {'ner': 3180.8612288131017}\n",
      "34 Losses {'ner': 2870.54256662822}\n",
      "35 Losses {'ner': 2733.0919920298425}\n",
      "36 Losses {'ner': 2720.685593548159}\n",
      "37 Losses {'ner': 2868.5006923633473}\n",
      "38 Losses {'ner': 2528.235601844719}\n",
      "39 Losses {'ner': 2357.02237563536}\n",
      "40 Losses {'ner': 2327.0757297021055}\n",
      "41 Losses {'ner': 2315.988309135843}\n",
      "42 Losses {'ner': 2424.4997185054563}\n",
      "43 Losses {'ner': 2359.4566958640175}\n",
      "44 Losses {'ner': 2203.0826161514733}\n",
      "45 Losses {'ner': 2157.034283520184}\n",
      "46 Losses {'ner': 2106.129465855781}\n",
      "47 Losses {'ner': 2096.156715077839}\n",
      "48 Losses {'ner': 2061.5363728781003}\n",
      "49 Losses {'ner': 1904.340810825825}\n",
      "50 Losses {'ner': 1877.4951734715685}\n",
      "51 Losses {'ner': 1958.0205531655506}\n",
      "52 Losses {'ner': 2061.976154819653}\n",
      "53 Losses {'ner': 2002.3984536224796}\n",
      "54 Losses {'ner': 1770.8520726309398}\n",
      "55 Losses {'ner': 1859.346637675035}\n",
      "56 Losses {'ner': 1724.1989830238758}\n",
      "57 Losses {'ner': 1679.5074359386513}\n",
      "58 Losses {'ner': 1913.9296813757685}\n",
      "59 Losses {'ner': 1466.3402120254548}\n",
      "60 Losses {'ner': 1828.590739226441}\n",
      "61 Losses {'ner': 1635.4857501475944}\n",
      "62 Losses {'ner': 1616.5601630809133}\n",
      "63 Losses {'ner': 1656.735226199154}\n",
      "64 Losses {'ner': 1657.873358965196}\n",
      "65 Losses {'ner': 1540.832387237839}\n",
      "66 Losses {'ner': 1480.2851637129686}\n",
      "67 Losses {'ner': 1475.860279848367}\n",
      "68 Losses {'ner': 1525.2032776782885}\n",
      "69 Losses {'ner': 1404.4963078304552}\n",
      "70 Losses {'ner': 1454.621749271041}\n",
      "71 Losses {'ner': 1299.9801711207492}\n",
      "72 Losses {'ner': 1388.952378403285}\n",
      "73 Losses {'ner': 1277.3683468826246}\n",
      "74 Losses {'ner': 1316.1873428006952}\n",
      "75 Losses {'ner': 1160.9929936619653}\n",
      "76 Losses {'ner': 1242.8438474946854}\n",
      "77 Losses {'ner': 1258.05471252093}\n",
      "78 Losses {'ner': 1114.886524973954}\n",
      "79 Losses {'ner': 1199.9347483513025}\n",
      "\n",
      "Saved model to /content/drive/MyDrive/qubitrics_internship_assignment/model\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner, last=True)\n",
    "\n",
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "nlp.begin_training()\n",
    "for itn in range(n_iter):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.01))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        try:\n",
    "            nlp.update(\n",
    "                texts,\n",
    "                annotations, \n",
    "                drop=0.6,\n",
    "                losses=losses,\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "    print(f\"{itn} Losses\", losses)\n",
    "\n",
    "output_dir = Path(output_dir)\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir()\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"\\nSaved model to\", output_dir)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "train_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (tags/v3.8.8:024d805, Feb 19 2021, 13:18:16) [MSC v.1928 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "48d00802732455b9462ba8716cf9ff3927992ea049ac0453fb26b28cf35689fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
